from collections import Counter
users = ["bobby1","bobby2","bobby3","bobby1","bobby22","bobby3","bobby4","bobby2","bobby3","bobby3"]
user_counter=Counter(users)
print(user_counter)

user =Counter("abubuwudwijdiwjdw")
print(user)
# 更新或者说是添加的操作
user_counter.update(user)
print(user_counter)
'''
Top n
问题：
假如存在一个很大的文件，文件中的每一行是一个字符串。
请问在内存有限的情况下（内存无法加载这个文件中的所有内容），
如何计算出出现频率最高的前100名字符串？
由于面试之前在学习hbase时了解了一下布隆过滤器（对bitmap的扩展使用），
所以当时立刻想到用bitmap去解决这个问题，但是考虑到bitmap无法对出现的单词计数的。所以我当时的回答是：
首先实例化一个数组，然后读取文件对每一行的字符串进行hash得到一个数值，然后将数组下标为这个数值的值+1，
最后再将这个数组排序并取出前100名。


接下来总结一下正确的答案，其实是在我看了编程珠玑的第一张后想到的答案。如果早些看了这本书那么这次面试可能就不会这么糟糕了。
在看了第一张以后，我对大数据的处理方法的总结是四个字：分而治之。所谓分而治之就是把大文件拆分成多个内存能够一次性容纳的小文件，
然后依次处理所有小文件，最后对小文件归并处理。具体到这个面试题：
假设分成n个小文件。读取大文件，对每一行hash得到hash值h，把这行写入第h/n个文件中。这样做是为了保证相同的字符串一定被分到相同的文件中。
如果小文件依然比内存大，那么我们再对其进行相同的操作（把小文件分成更小的文件，直到可以被内存一次性装载为止）。
对所有小文件进行统计计算（如利用HashMap）并排序，然后将结果写入新的文件。文件内容是：字符串，出现次数。
读取所有新生成的文件中的前几条记录并排序，最后输出top N。
'''
# 取出最多的前几个的数
print(user_counter.most_common(2))